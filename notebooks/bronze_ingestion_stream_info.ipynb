{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a28c5e1-2e71-4669-a7f9-20ac80709988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Workspace/Users/laugur1508@gmail.com/twitch_data_pipeline/notebooks')\n",
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp, to_timestamp, concat, col, lit\n",
    "import logging\n",
    "import json\n",
    "from utils.auth import get_access_token, twitch_api_request\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, TimestampType\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "streams_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"user_login\", StringType(), True),\n",
    "    StructField(\"user_name\", StringType(), True),\n",
    "    StructField(\"game_id\", StringType(), True),\n",
    "    StructField(\"game_name\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"viewer_count\", IntegerType(), True),\n",
    "    StructField(\"started_at\", StringType(), True),  # string to keep original format, can cast later\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"thumbnail_url\", StringType(), True),\n",
    "    StructField(\"tag_ids\", StringType(), True),\n",
    "    StructField(\"tags\", StringType(), True),\n",
    "    StructField(\"is_mature\", BooleanType(), True),\n",
    "    StructField(\"ingestion_time\", TimestampType(), True),\n",
    "    StructField(\"ingestion_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "def get_twitch_streams_fixed(streamers, client_id, access_token):\n",
    "    url = \"https://api.twitch.tv/helix/streams\"\n",
    "    batch_size = 10\n",
    "    all_streams = []\n",
    "\n",
    "    for i in range(0, len(streamers), batch_size):\n",
    "        batch = streamers[i:i + batch_size]\n",
    "        params = [(\"user_login\", s) for s in batch]\n",
    "\n",
    "        try:\n",
    "            streams = twitch_api_request(url, client_id, access_token, params)\n",
    "            streams = streams.get('data', [])\n",
    "            if streams:\n",
    "                logger.info(f\"Batch {i // batch_size + 1}: Retrieved {len(streams)} active streams\")\n",
    "                all_streams.extend(streams)\n",
    "            else:\n",
    "                logger.warning(f\"Batch {i // batch_size + 1}: No streams or error occurred\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch {i // batch_size + 1} failed: {e}\")\n",
    "\n",
    "    return all_streams\n",
    "\n",
    "def clean_and_standardize_data(streams_data):\n",
    "    if not streams_data:\n",
    "        return None\n",
    "\n",
    "    pdf = pd.DataFrame(streams_data)\n",
    "\n",
    "    # Convert list fields to comma-separated strings\n",
    "    for col_name in ['tag_ids', 'tags']:\n",
    "        if col_name in pdf.columns:\n",
    "            pdf[col_name] = pdf[col_name].apply(lambda x: ','.join(x) if isinstance(x, list) and x else '')\n",
    "        else:\n",
    "            pdf[col_name] = ''\n",
    "\n",
    "    expected_columns = {\n",
    "        'id': str, 'user_id': str, 'user_login': str, 'user_name': str,\n",
    "        'game_id': str, 'game_name': str, 'type': str, 'title': str,\n",
    "        'viewer_count': int, 'started_at': str, 'language': str,\n",
    "        'thumbnail_url': str, 'tag_ids': str, 'tags': str, 'is_mature': bool\n",
    "    }\n",
    "\n",
    "    for col_name, col_type in expected_columns.items():\n",
    "        if col_name not in pdf.columns:\n",
    "            if col_type == str:\n",
    "                pdf[col_name] = ''\n",
    "            elif col_type == int:\n",
    "                pdf[col_name] = 0\n",
    "            elif col_type == bool:\n",
    "                pdf[col_name] = False\n",
    "\n",
    "    for col_name, col_type in expected_columns.items():\n",
    "        try:\n",
    "            if col_type == str:\n",
    "                pdf[col_name] = pdf[col_name].astype(str).fillna('')\n",
    "            elif col_type == int:\n",
    "                pdf[col_name] = pd.to_numeric(pdf[col_name], errors='coerce').fillna(0).astype(int)\n",
    "            elif col_type == bool:\n",
    "                pdf[col_name] = pdf[col_name].astype(bool)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not convert column {col_name} to {col_type}: {e}\")\n",
    "\n",
    "    return pdf\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_streams\",\n",
    "    comment=\"Bronze layer: Raw Twitch streams data with append-only mode\",\n",
    "    table_properties={\n",
    "        \"delta.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.autoOptimize.autoCompact\": \"true\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    },\n",
    "    table_type=\"live\"\n",
    ")\n",
    "def bronze_streams():\n",
    "    \"\"\"\n",
    "    Bronze layer table for Twitch streams data.\n",
    "    Uses append mode to ensure data is not overwritten.\n",
    "    Captures all stream snapshots over time.\n",
    "    \"\"\"\n",
    "    # Retrieve API credentials\n",
    "    CLIENT_ID = dbutils.secrets.get(scope=\"my-secret\", key=\"CLIENT-ID\")\n",
    "    CLIENT_SECRET = dbutils.secrets.get(scope=\"my-secret\", key=\"CLIENT-SECRET\")\n",
    "    ACCESS_TOKEN = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
    "    \n",
    "    if not ACCESS_TOKEN:\n",
    "        logger.error(\"Failed to get Twitch access token\")\n",
    "        return spark.createDataFrame([], streams_schema)\n",
    "\n",
    "    # Load streamer configuration\n",
    "    config_path = \"dbfs:/FileStore/config/top50FrenchStreamer.json\"\n",
    "    config = json.loads(dbutils.fs.head(config_path))\n",
    "    top50FrenchStreamers = config[\"top50FrenchStreamers\"]\n",
    "\n",
    "    logger.info(f\"Starting Twitch streams ETL for {len(top50FrenchStreamers)} streamers.\")\n",
    "\n",
    "    # Fetch streams data from Twitch API\n",
    "    streams = get_twitch_streams_fixed(top50FrenchStreamers, CLIENT_ID, ACCESS_TOKEN)\n",
    "\n",
    "    if not streams:\n",
    "        logger.info(\"No active streams found.\")\n",
    "        return spark.createDataFrame([], streams_schema)\n",
    "\n",
    "    # Clean and standardize the data\n",
    "    cleaned_pdf = clean_and_standardize_data(streams)\n",
    "    if cleaned_pdf is None or cleaned_pdf.empty:\n",
    "        logger.warning(\"No data to process after cleaning.\")\n",
    "        return spark.createDataFrame([], streams_schema)\n",
    "\n",
    "    # Convert to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(cleaned_pdf)\n",
    "\n",
    "    # Add timestamp conversion and metadata columns\n",
    "    spark_df = spark_df.withColumn(\"started_at\", to_timestamp(\"started_at\")) \\\n",
    "                       .withColumn(\"ingestion_time\", current_timestamp()) \\\n",
    "                       .withColumn(\"ingestion_date\", current_timestamp().cast(\"date\").cast(\"string\"))\n",
    "\n",
    "    # Add a unique identifier to prevent duplicates\n",
    "    spark_df = spark_df.withColumn(\"record_id\", \n",
    "                                   concat(col(\"user_id\"), \n",
    "                                         lit(\"_\"), \n",
    "                                         col(\"started_at\").cast(\"string\")))\n",
    "\n",
    "    logger.info(f\"Successfully processed {spark_df.count()} stream records\")\n",
    "    return spark_df  # Fixed: removed spark.readStream()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion_stream_info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
